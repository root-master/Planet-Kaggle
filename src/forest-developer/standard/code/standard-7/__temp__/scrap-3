/opt/anaconda3/bin/python3.6 /root/share/project/pytorch/build/satndard-7/train-forest-0.py
train-forest-0.py: calling main function ...

--- [START 2017-05-30 22:06:27] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 123
	file    = /root/share/project/pytorch/build/satndard-7/train-forest-0.py
	out_dir = /root/share/project/pytorch/results/kaggle-forest/resnet18-pretrain-40479-jpg-xxx

** dataset setting **
	(height,width)    = (256, 256)
	in_channels       = 3
	train_dataset.split = train-3000
	train_dataset.num = 3000
	test_dataset.split = valid-8000
	test_dataset.num  = 8000
	batch_size        = 96
	train_loader.sampler = <torch.utils.data.sampler.RandomSampler object at 0x7f3ba52a4898>
	test_loader.sampler  = <torch.utils.data.sampler.SequentialSampler object at 0x7f3b17c46710>

** net setting **
<class 'net.model.resnet.ResNet'>

    def __init__(self, block, layers, in_shape=(3,244,244), num_classes=1000):
        self.inplanes = 64

        super(ResNet, self).__init__()
        in_channels, height, width = in_shape

        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1  = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        #self.pool = nn.AvgPool2d(kernel_size=7)
        #self.pool = nn.AdaptiveAvgPool2d(1)
        #self.pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        ##x8  = F.dropout(x8,p=0.5,training=self.training)
        ##x9  = self.pool(x8)
        #x = F.adaptive_avg_pool2d(x,output_size=1) + F.adaptive_max_pool2d(x,output_size=1)
        #x9 = F.adaptive_max_pool2d(x8,output_size=1)
        x = F.adaptive_avg_pool2d(x,output_size=1)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        logit = x
        prob  = F.sigmoid(logit)
        return logit, prob


** start training here! **
 optimizer=<torch.optim.sgd.SGD object at 0x7f3ba4c88cf8>
 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0      31    0.1000   |  0.289  | 0.234  0.759 | 0.306  0.697  |  0.1 min
  2.0      31    0.1000   |  0.203  | 0.214  0.810 | 0.263  0.721  |  0.1 min
  3.0      31    0.1000   |  0.183  | 0.201  0.786 | 0.218  0.773  |  0.1 min
  4.0      31    0.1000   |  0.179  | 0.176  0.812 | 0.194  0.781  |  0.1 min
  5.0      31    0.1000   |  0.171  | 0.147  0.873 | 0.231  0.762  |  0.1 min
  6.0      31    0.1000   |  0.170  | 0.198  0.791 | 0.190  0.797  |  0.1 min
  7.0      31    0.1000   |  0.162  | 0.168  0.836 | 0.349  0.698  |  0.1 min
  8.0      31    0.1000   |  0.161  | 0.187  0.814 | 0.462  0.697  |  0.1 min
  9.0      31    0.1000   |  0.163  | 0.172  0.835 | 0.176  0.814  |  0.1 min
 10.0      31    0.1000   |  0.156  | 0.179  0.829 | 0.261  0.744  |  0.1 min
 11.0      31    0.0100   |  0.150  | 0.125  0.862 | 0.156  0.839  |  0.1 min
 12.0      31    0.0100   |  0.140  | 0.129  0.870 | 0.149  0.856  |  0.1 min
 13.0      31    0.0100   |  0.142  | 0.142  0.869 | 0.143  0.860  |  0.1 min
 14.0      31    0.0100   |  0.142  | 0.167  0.836 | 0.145  0.861  |  0.1 min
 15.0      31    0.0100   |  0.137  | 0.134  0.862 | 0.145  0.862  |  0.1 min
 16.0      31    0.0100   |  0.138  | 0.149  0.861 | 0.177  0.811  |  0.1 min
 17.0      31    0.0100   |  0.138  | 0.141  0.867 | 0.142  0.865  |  0.1 min
 18.0      31    0.0100   |  0.137  | 0.139  0.844 | 0.146  0.861  |  0.1 min
 19.0      31    0.0100   |  0.137  | 0.139  0.861 | 0.149  0.852  |  0.1 min
 20.0      31    0.0100   |  0.135  | 0.143  0.857 | 0.182  0.824  |  0.1 min
 21.0      31    0.0100   |  0.132  | 0.133  0.876 | 0.138  0.869  |  0.1 min
 22.0      31    0.0100   |  0.135  | 0.112  0.905 | 0.157  0.851  |  0.1 min
 23.0      31    0.0100   |  0.132  | 0.131  0.888 | 0.176  0.833  |  0.1 min
 24.0      31    0.0100   |  0.133  | 0.147  0.857 | 0.147  0.856  |  0.1 min
 25.0      31    0.0100   |  0.134  | 0.124  0.875 | 0.138  0.866  |  0.1 min
 26.0      31    0.0050   |  0.130  | 0.100  0.906 | 0.137  0.871  |  0.1 min
 27.0      31    0.0050   |  0.129  | 0.117  0.895 | 0.137  0.868  |  0.1 min
 28.0      31    0.0050   |  0.131  | 0.123  0.879 | 0.136  0.870  |  0.1 min
 29.0      31    0.0050   |  0.130  | 0.126  0.868 | 0.140  0.868  |  0.1 min
 30.0      31    0.0050   |  0.130  | 0.108  0.911 | 0.138  0.869  |  0.1 min
 31.0      31    0.0050   |  0.132  | 0.133  0.878 | 0.152  0.856  |  0.1 min
 32.0      31    0.0050   |  0.129  | 0.125  0.887 | 0.139  0.867  |  0.1 min
 33.0      31    0.0050   |  0.128  | 0.109  0.895 | 0.139  0.863  |  0.1 min
 34.0      31    0.0050   |  0.127  | 0.139  0.885 | 0.174  0.818  |  0.1 min
 35.0      31    0.0050   |  0.128  | 0.125  0.881 | 0.136  0.870  |  0.1 min
 36.0      31    0.0010   |  0.127  | 0.142  0.877 | 0.133  0.875  |  0.1 min
 37.0      31    0.0010   |  0.126  | 0.110  0.907 | 0.138  0.864  |  0.1 min
 38.0      31    0.0010   |  0.129  | 0.122  0.872 | 0.140  0.861  |  0.1 min
 39.0      31    0.0010   |  0.126  | 0.131  0.862 | 0.133  0.875  |  0.1 min
 40.0      31    0.0010   |  0.127  | 0.109  0.905 | 0.132  0.875  |  0.1 min
 41.0      31    0.0001   |  0.125  | 0.124  0.902 | 0.132  0.875  |  0.1 min
 42.0      31    0.0001   |  0.128  | 0.144  0.882 | 0.132  0.875  |  0.1 min
 43.0      31    0.0001   |  0.126  | 0.126  0.893 | 0.132  0.875  |  0.1 min

/root/share/project/pytorch/results/kaggle-forest/resnet18-pretrain-40479-jpg-xxx/snap/final.torch:
	all time to train=9.6 min
	test_loss=0.132443, test_acc=0.874818

sucess!

Process finished with exit code 0
/opt/anaconda3/bin/python3.6 /root/share/project/pytorch/build/satndard-7/train-forest-0.py
train-forest-0.py: calling main function ...

--- [START 2017-05-30 22:59:50] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 123
	file    = /root/share/project/pytorch/build/satndard-7/train-forest-0.py
	out_dir = /root/share/project/pytorch/results/kaggle-forest/resnet18-pretrain-40479-jpg-xx1

** dataset setting **
	(height,width)    = (256, 256)
	in_channels       = 3
	train_dataset.split = train-3000
	train_dataset.num = 3000
	test_dataset.split = valid-8000
	test_dataset.num  = 8000
	batch_size        = 96
	train_loader.sampler = <torch.utils.data.sampler.RandomSampler object at 0x7f26cfbce940>
	test_loader.sampler  = <torch.utils.data.sampler.SequentialSampler object at 0x7f26cfb757b8>

** net setting **
<class 'net.model.resnet.ResNet'>

    def __init__(self, block, layers, in_shape=(3,244,244), num_classes=1000):
        self.inplanes = 64

        super(ResNet, self).__init__()
        in_channels, height, width = in_shape

        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1  = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        #self.pool = nn.AvgPool2d(kernel_size=7)
        #self.pool = nn.AdaptiveAvgPool2d(1)
        #self.pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        ##x8  = F.dropout(x8,p=0.5,training=self.training)
        ##x9  = self.pool(x8)
        #x = F.adaptive_avg_pool2d(x,output_size=1) + F.adaptive_max_pool2d(x,output_size=1)
        #x9 = F.adaptive_max_pool2d(x8,output_size=1)
        x = F.adaptive_avg_pool2d(x,output_size=1)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        logit = x
        prob  = F.sigmoid(logit)
        return logit, prob


** start training here! **
 optimizer=<torch.optim.sgd.SGD object at 0x7f2642ef3dd8>
 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0      31    0.0100   |  0.366  | 0.241  0.742 | 0.225  0.758  |  0.1 min
  2.0      31    0.0100   |  0.194  | 0.177  0.806 | 0.176  0.827  |  0.1 min
  3.0      31    0.0100   |  0.165  | 0.157  0.868 | 0.154  0.855  |  0.1 min
  4.0      31    0.0100   |  0.150  | 0.146  0.862 | 0.146  0.865  |  0.1 min
  5.0      31    0.0100   |  0.139  | 0.135  0.886 | 0.139  0.872  |  0.1 min
  6.0      31    0.0100   |  0.133  | 0.144  0.883 | 0.139  0.870  |  0.1 min
  7.0      31    0.0100   |  0.127  | 0.130  0.876 | 0.128  0.882  |  0.1 min
  8.0      31    0.0100   |  0.120  | 0.143  0.873 | 0.122  0.891  |  0.1 min
  9.0      31    0.0100   |  0.119  | 0.099  0.916 | 0.129  0.889  |  0.1 min
 10.0      31    0.0100   |  0.115  | 0.115  0.902 | 0.123  0.883  |  0.1 min
 11.0      31    0.0050   |  0.115  | 0.117  0.903 | 0.115  0.898  |  0.1 min
 12.0      31    0.0050   |  0.111  | 0.112  0.907 | 0.113  0.900  |  0.1 min
 13.0      31    0.0050   |  0.110  | 0.128  0.884 | 0.118  0.894  |  0.1 min
 14.0      31    0.0050   |  0.108  | 0.105  0.909 | 0.112  0.902  |  0.1 min
 15.0      31    0.0050   |  0.108  | 0.112  0.901 | 0.111  0.902  |  0.1 min
 16.0      31    0.0050   |  0.106  | 0.102  0.912 | 0.112  0.899  |  0.1 min
 17.0      31    0.0050   |  0.103  | 0.096  0.921 | 0.110  0.901  |  0.1 min
 18.0      31    0.0050   |  0.107  | 0.123  0.901 | 0.113  0.900  |  0.1 min
 19.0      31    0.0050   |  0.103  | 0.101  0.917 | 0.111  0.902  |  0.1 min
 19.6      20    0.0050   |  0.103  | 0.088  0.932 | ...


 #inceptionv2 deep finetinue

 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0      62    0.0100   |  0.228  | 0.205  0.778 | 0.318  0.661  |  0.6 min
  2.0      62    0.0100   |  0.190  | 0.204  0.802 | 0.314  0.665  |  0.6 min
  3.0      62    0.0100   |  0.157  | 0.145  0.874 | 0.296  0.693  |  0.6 min
  4.0      62    0.0100   |  0.138  | 0.101  0.894 | 0.289  0.704  |  0.6 min


fc only
 epoch   iter   rate  |  smooth_loss   |  train_loss  (acc)  |  valid_loss  (acc)  | min
----------------------------------------------------------------------------------------
  1.0      62    0.0100   |  0.256  | 0.230  0.773 | 0.349  0.623  |  0.6 min
  2.0      62    0.0100   |  0.234  | 0.241  0.730 | 0.341  0.620  |  0.6 min
  3.0      62    0.0100   |  0.224  | 0.202  0.835 | 0.334  0.618  |  0.6 min
  4.0      62    0.0100   |  0.213  | 0.219  0.815 | 0.331  0.620  |  0.6 min
  5.0      62    0.0100   |  0.207  | 0.220  0.787 | 0.327  0.632  |  0.6 min
  6.0      62    0.0100   |  0.196  | 0.189  0.801 | 0.322  0.644  |  0.7 min
Process Process-39:

----------------------------------------------------------------------------------------
  1.0      62    0.1000   |  0.147  | 0.150  0.861 | 0.592  0.290  |  0.7 min
  2.0      62    0.1000   |  0.131  | 0.095  0.943 | 0.498  0.410  |  0.7 min
  3.0      62    0.1000   |  0.118  | 0.113  0.891 | 0.408  0.628  |  0.7 min
  4.0      62    0.1000   |  0.109  | 0.105  0.916 | 0.330  0.609  |  0.7 min
  5.0      62    0.1000   |  0.113  | 0.128  0.882 | 0.357  0.609  |  0.7 min
  6.0      62    0.1000   |  0.110  | 0.106  0.890 | 0.403  0.549  |  0.7 min
  7.0      62    0.1000   |  0.103  | 0.092  0.916 | 0.436  0.501  |  0.7 min
  8.0      62    0.1000   |  0.105  | 0.114  0.867 | 0.459  0.484  |  0.7 min
  9.0      62    0.1000   |  0.093  | 0.086  0.917 | 0.457  0.484  |  0.7 min
 10.0      62    0.1000   |  0.090  | 0.087  0.903 | 0.476  0.484  |  0.7 min
 11.0      62    0.0100   |  0.085  | 0.079  0.902 | 0.474  0.484  |  0.7 min
 12.0      62    0.0100   |  0.073  | 0.096  0.919 | 0.471  0.484  |  0.7 min
 13.0      62    0.0100   |  0.071  | 0.078  0.932 | 0.471  0.484  |  0.7 min
 14.0      62    0.0100   |  0.072  | 0.068  0.960 | 0.467  0.484  |  0.7 min
 15.0      62    0.0100   |  0.068  | 0.060  0.956 | 0.470  0.484  |  0.7 min
 16.0      62    0.0100   |  0.065  | 0.086  0.925 | 0.465  0.484  |  0.7 min
 17.0      62    0.0100   |  0.065  | 0.078  0.944 | 0.463  0.484  |  0.7 min
Process Process-105: